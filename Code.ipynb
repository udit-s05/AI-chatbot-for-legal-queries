{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d8fa11-b551-46b2-abc9-b93dd1d6e177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model/tokenizer: C:/Users/udits/Downloads/finetuned-gpt2-20241114T151321Z-001 does not appear to have a file named config.json. Checkout 'https://huggingface.co/C:/Users/udits/Downloads/finetuned-gpt2-20241114T151321Z-001/tree/main' for available files.\n",
      "Welcome to the Law Chatbot! Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a legal question (or type 'exit' to quit):  equal wages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer is not loaded!\n",
      "Lawbot response: None\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a legal question (or type 'exit' to quit):  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer is not loaded!\n",
      "Lawbot response: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model_path = \"C:/Users/udits/Downloads/finetuned-gpt2-20241114T151321Z-001\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading the model and tokenizer once\n",
    "try:\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model/tokenizer: {e}\")\n",
    "\n",
    "def generate_response(prompt):\n",
    "    # Ensure tokenizer is defined before using\n",
    "    if 'tokenizer' not in globals():\n",
    "        print(\"Tokenizer is not loaded!\")\n",
    "        return\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,  # Increased max length\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.strip()  # Cleaner output\n",
    "\n",
    "# Law chatbot loop\n",
    "print(\"Welcome to the Law Chatbot! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"Ask a legal question (or type 'exit' to quit): \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Exiting the chatbot...\")\n",
    "        break\n",
    "    response = generate_response(user_input)\n",
    "    print(f\"Lawbot response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70b07407-6b28-41c7-afcb-229ff8f0c609",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer, GPT2LMHeadModel\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load fine-tuned model and tokenizer\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetuned-gpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import gradio as gr\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model_path = \"finetuned-gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Function to generate responses\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)  # Ensure padding is applied\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,  # Increased length for more context if needed\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Ensure pad_token_id is set\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.strip()  # Return cleaned response\n",
    "\n",
    "# Set up Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=generate_response,  # Function to call\n",
    "    inputs=\"text\",  # Input type\n",
    "    outputs=\"text\",  # Output type\n",
    "    title=\"Law Chatbot\",\n",
    "    description=\"Ask a legal question and get an AI-generated response. Type a question below to get started.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a7ce7-0475-42bf-aec1-c25a745be070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
